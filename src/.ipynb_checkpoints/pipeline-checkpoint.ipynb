{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ea157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错误: 无法从 'src/models.py' 导入。\n",
      "请确保这个 Notebook 和 'src' 文件夹在同一个根目录下。\n",
      "--- 步骤 1: 加载模型 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# ## 3. 步骤 1: 加载模型\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 加载 SAM 和 CLIP。这可能需要一些时间，特别是 SAM checkpoint（约 2.4GB）需要下载时。\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- 步骤 1: 加载模型 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m clip_model, clip_preprocess, clip_device \u001b[38;5;241m=\u001b[39m \u001b[43mload_clip\u001b[49m()\n\u001b[1;32m    143\u001b[0m mask_generator, sam_device \u001b[38;5;241m=\u001b[39m load_sam_automask_generator()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([clip_model, mask_generator]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_clip' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    from models import load_clip, load_sam_automask_generator, get_device\n",
    "except ImportError:\n",
    "    print(\"错误: 无法从 'src/models.py' 导入。\")\n",
    "    print(\"请确保这个 Notebook 和 'src' 文件夹在同一个根目录下。\")\n",
    "\n",
    "\n",
    "TEST_IMAGE_PATH = \"samclip_project/images/truck.jpg\"\n",
    "\n",
    "def download_test_image(target_path=TEST_IMAGE_PATH, url=TEST_IMAGE_URL):\n",
    "    \"\"\"\n",
    "    如果测试图片不存在，则下载它。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"测试图片 '{target_path}' 未找到，开始下载...\")\n",
    "        try:\n",
    "            urlretrieve(url, target_path)\n",
    "            print(f\"测试图片下载完成: {target_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"下载测试图片失败: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"测试图片 '{target_path}' 已存在。\")\n",
    "    return True\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    加载图片并转换为 RGB 格式。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"图片文件未找到: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"无法读取图片: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591e600-f155-47f7-bedb-940a0085af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_to_image(image, mask):\n",
    "    \"\"\"\n",
    "    将二进制蒙版应用于图像，裁剪出蒙版区域。\n",
    "    返回一个 PIL Image，以便 CLIP 预处理器使用。\n",
    "    \"\"\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.squeeze()\n",
    "    masked_image = np.zeros((*image.shape[:2], 4), dtype=np.uint8)\n",
    "    masked_image[..., :3] = image\n",
    "    masked_image[mask, 3] = 255\n",
    "    return Image.fromarray(masked_image, 'RGBA')\n",
    "\n",
    "def show_anns(image, anns, title=\"\"):\n",
    "    \"\"\"\n",
    "    在图像上显示所有 SAM 蒙版。\n",
    "    \"\"\"\n",
    "    if not anns:\n",
    "        print(\"没有找到蒙版。\")\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['predicted_iou']), reverse=True)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_best_mask(image, mask, score, text_prompt):\n",
    "    \"\"\"\n",
    "    显示原始图像和得分最高的蒙版。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"原始图像\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    result_image = np.zeros_like(image)\n",
    "    result_image[mask] = image[mask]\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(f\"结果: \\\"{text_prompt}\\\"\\nCLIP Score: {score:.4f}\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. 步骤 1: 加载模型\n",
    "#\n",
    "# 加载 SAM 和 CLIP。这可能需要一些时间，特别是 SAM checkpoint（约 2.4GB）需要下载时。\n",
    "\n",
    "# %%\n",
    "print(\"--- 步骤 1: 加载模型 ---\")\n",
    "clip_model, clip_preprocess, clip_device = load_clip()\n",
    "mask_generator, sam_device = load_sam_automask_generator()\n",
    "\n",
    "if not all([clip_model, mask_generator]):\n",
    "    print(\"模型加载失败，请检查 'src/models.py' 的设置和 checkpoint 路径。\")\n",
    "else:\n",
    "    print(\"--- 模型加载完毕 ---\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. 步骤 2: 加载图像\n",
    "#\n",
    "# 下载（如果需要）并加载我们的测试图像。\n",
    "\n",
    "# %%\n",
    "print(\"--- 步骤 2: 加载图像 ---\")\n",
    "download_test_image(TEST_IMAGE_PATH, TEST_IMAGE_URL)\n",
    "image_rgb = load_image(TEST_IMAGE_PATH)\n",
    "\n",
    "if image_rgb is not None:\n",
    "    print(f\"图像 '{TEST_IMAGE_PATH}' 加载成功，形状: {image_rgb.shape}\")\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(\"原始测试图像\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"无法加载图像: {TEST_IMAGE_PATH}，终止流程。\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. 步骤 3: SAM 生成所有蒙版\n",
    "#\n",
    "# 运行 `SamAutomaticMaskGenerator`。这会找到图像中所有可能的“事物”并为它们生成蒙版。\n",
    "#\n",
    "# （这一步可能需要几秒钟到一分钟，取决于你的设备）\n",
    "\n",
    "# %%\n",
    "print(\"--- 步骤 3: SAM 正在生成所有蒙版... ---\")\n",
    "# SAM generator 需要 BGR 格式的图像\n",
    "image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "sam_masks = mask_generator.generate(image_bgr)\n",
    "\n",
    "if not sam_masks:\n",
    "    print(\"SAM 未能生成任何蒙版。\")\n",
    "else:\n",
    "    print(f\"SAM 生成了 {len(sam_masks)} 个蒙版。\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### (可选) 可视化 SAM 的所有输出\n",
    "#\n",
    "# 让我们看看 SAM 到底找到了多少东西。\n",
    "\n",
    "# %%\n",
    "# 可选：显示 SAM 生成的所有蒙版\n",
    "print(\"正在显示所有 SAM 蒙版...\")\n",
    "show_anns(image_rgb, sam_masks, title=f\"SAM 生成的 {len(sam_masks)} 个蒙版\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. 步骤 4: 定义语言提示\n",
    "#\n",
    "# 这是我们项目的核心。你想要分割什么？\n",
    "\n",
    "# %%\n",
    "# 试试 \"a blue truck\", \"a wheel\", \"the driver's window\"\n",
    "text_prompt = \"a blue truck\"\n",
    "\n",
    "print(f\"--- 步骤 4: CLIP 正在处理文本提示... ---\")\n",
    "text = clip.tokenize([text_prompt]).to(clip_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"文本提示: \\\"{text_prompt}\\\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. 步骤 5 & 6: CLIP 评分\n",
    "#\n",
    "# 我们将循环遍历 SAM 生成的 *每一个* 蒙版：\n",
    "# 1.  用蒙版裁剪（Crop）原始图像。\n",
    "# 2.  将裁剪后的图像送入 CLIP 进行预处理。\n",
    "# 3.  将所有处理后的图像打包（Batch）。\n",
    "# 4.  计算文本特征和所有图像特征之间的相似度分数。\n",
    "\n",
    "# %%\n",
    "print(f\"--- 步骤 5: CLIP 正在处理所有 {len(sam_masks)} 个蒙版图像... ---\")\n",
    "\n",
    "processed_images = []\n",
    "\n",
    "for ann in sam_masks:\n",
    "    mask = ann['segmentation'] # (H, W) boolean 数组\n",
    "    cropped_pil_image = apply_mask_to_image(image_rgb, mask)\n",
    "    processed_images.append(clip_preprocess(cropped_pil_image))\n",
    "\n",
    "# 创建一个 batch\n",
    "image_batch = torch.stack(processed_images).to(clip_device)\n",
    "print(f\"创建了 {len(image_batch)} 张裁剪图像的 Batch。\")\n",
    "\n",
    "print(\"\\n--- 步骤 6: CLIP 正在计算相似度分数... ---\")\n",
    "with torch.no_grad():\n",
    "    # 编码所有裁剪后的图像\n",
    "    image_features = clip_model.encode_image(image_batch)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # 计算文本特征和所有图像特征之间的余弦相似度\n",
    "    similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
    "    \n",
    "# (1, N) -> (N,)\n",
    "scores = similarity.squeeze()\n",
    "print(\"分数计算完毕。\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. 步骤 7 & 8: 查找并显示最佳结果\n",
    "#\n",
    "# 找到得分最高的蒙版并将其显示出来。\n",
    "\n",
    "# %%\n",
    "print(\"--- 步骤 7: 查找最佳匹配... ---\")\n",
    "best_score, best_index = torch.max(scores, 0)\n",
    "best_mask_ann = sam_masks[best_index.item()]\n",
    "best_mask = best_mask_ann['segmentation'] # (H, W) boolean 数组\n",
    "\n",
    "print(f\"找到最佳蒙版，索引: {best_index.item()}, 分数: {best_score.item():.4f}\")\n",
    "\n",
    "print(\"\\n--- 步骤 8: 显示结果 ---\")\n",
    "show_best_mask(image_rgb, best_mask, best_score.item(), text_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f30b65-7b0e-41eb-99f4-06c50bc33ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
